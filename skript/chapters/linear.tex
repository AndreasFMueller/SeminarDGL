%
% linear.tex -- L"osung linearer Differentialgleichungen
%
% (c) 2015 Prof Dr Andreas Mueller, Hochschule Rapperswil
%
\chapter{Differentialgleichungen und lineare Algebra\label{chapter:linear}}
\lhead{}
\rhead{Lineare Differentialgleichungen}
Es lohnt sich, lineare Differentialgleichungen unter Zuhilfenahme
der linearen Algebra etwas genauer zu untersuchen.
Gel"ost werden soll das lineare Differentialgleichungssystem
\begin{equation}
\frac{d}{dt}x = A(t)x + f(t)\qquad x(0)=x_0
\label{linear:gleichung}
\end{equation}
gel"ost werden.
Darin ist $A(t)$ eine $n\times n$-Matrix, wir nehmen der Einfachheit
halber an, dass die Matrixelemente $a_{kl}(t)$ von $A(t)$ differenzierbare
Funktionen von $t$ sind.
Die Funktion $f(t)$ hat $n$-dimensionale Vektoren als Werte.
Falls $f(t)=0$ ist, ist die Gleichung~(\ref{linear:gleichung}) 
homogen.

\section{Konstante Koeffizienten}
In diesem Abschnitt nehmen wir an, dass $A(t)$ konstant ist, wir schreiben
einfach $A$ f"ur den konstanten Wert von $A(t)$.
\subsection{Diagonalisierbare Koeffizientenmatrix}
Besonders einfach w"are (\ref{linear:gleichung}) zu l"osen, wenn $A$
Diagonalform h"atte,
\[
A=\begin{pmatrix}
\lambda_1&         &      &         \\
         &\lambda_2&      &         \\
         &         &\ddots&         \\
         &         &      &\lambda_n
\end{pmatrix}.
\]
Durch Wechsel der Basis kann man diese Situation immer erreichen, wenn
$A$ diagonalisierbar ist.

In diesem Fall zerf"allt das Gleichungssystem in $n$ unabh"angige
lineare Differentialgleichungen erster Ordnung
\begin{align*}
\dot{x}_1&=\lambda_1x_1 + f_1(t)\\
\dot{x}_2&=\lambda_2x_2 + f_2(t)\\
&\;\vdots\\
\dot{x}_n&=\lambda_nx_n + f_n(t).
\end{align*}
Die homogenen Gleichungen lassen sich sofort l"osen:
\[
\dot{x}_i=\lambda_ix_i
\qquad
\Rightarrow
\qquad
x_i(t)=x_{0i}e^{\lambda_it}.
\]
Daraus kann man die L"osung des inhomogenen Systems mit Hilfe der
Variation der Konstanten l"osen:
\[
x_i(t)=C_i(t)e^{\lambda_i t}
\qquad
\Rightarrow
\qquad
\dot{x}_i(t)
=
\dot{C}_i(t)e^{\lambda_i t}+C_i(t)\lambda_ie^{\lambda_i t}
=
\lambda_i x_i(t) + \dot{C}_i(t)e^{\lambda_i t}
\]
Die $C_i(t)$ m"ussen also so gew"ahlt werden, dass
$\dot{C}_i(t)=f_i(t)e^{-\lambda_i t}$, es folgt
\[
C_i(t)=x_{0i}+\int_0^t C_i(\tau)e^{-\lambda_i \tau}\,d\tau
\]
und damit die L"osung
\[
x_i(t)
=
\biggl(x_{0i}+\int_0^t C_i(\tau)e^{-\lambda_i \tau}\,d\tau\biggr)
e^{\lambda_i t}.
\]
Diese L"osung hatten wir fr"uher bereits gefunden, doch wollen wir
dies auch noch in Matrixform schreiben.
Dazu setzen wir
\[
e^{At}=\begin{pmatrix}
e^{\lambda_1 t}&               &      &               \\
               &e^{\lambda_1 t}&      &               \\
               &               &\ddots&               \\
               &               &      &e^{\lambda_n t}
\end{pmatrix}.
\]
Mit dieser Schreibweise kann die L"osung geschrieben werden als
\[
x(t)
=
e^{At}
\biggl(x_0 + \int_0^t e^{-A\tau}f(\tau)\,d\tau\biggr)
\]

\subsection{Matrix-Exponentialfunktion}
Die L"osung im vorangegangenen Abschnitt konnte mit Hilfe der
Matrix-Exponentialfunktion besonders kompakt formuliert werden.
Allerdings war diese nur f"ur diagonalisierbare Matrizen definiert.
Wir k"onnten $e^A$ aber auch "uber die Potenzreihe definieren
\[
e^A=E+A+\frac12A+\frac1{3!}A^3+\frac1{4!} A^4+\dots
\]
Die Ableitung von $e^{At}$ nach $t$ ist
\begin{align*}
\frac{d}{dt}e^{At}
&=
\frac{d}{dt}\biggl(E+At+\frac1{2!}t^2A^2+\frac1{3!}t^3A^3+\dots\Biggr)
\\
&=
A+tA^2+\frac12t^2A^3+\frac1{3!}t^3A^4+\dots
\\
&=
\biggl(E+tA+\frac12t^2A^2+\frac1{3!}A^3+\dots\biggr)A
=e^{tA}A=Ae^{At}
\end{align*}
Die Matrix-Funktion $t\mapsto X(t)=e^{At}$ erf"ullt also die
Matrix-Differentialgleichung
\begin{equation}
\frac{d}{dt} X = AX,
\label{linear:matrix-dgl}
\end{equation}

\subsection{Beliebige Koeffizientenmatrix}
Mit der Matrix-Exponentialfunktion kann jetzt auch das homogene
Differentialgleichungssystem gel"ost werden.
Dazu multiplizieren wir~(\ref{linear:matrix-dgl}) auf der rechten Seite
$x_0$ 
\[
\frac{d}{dt}X(t)x_0 = A X(t)x_0,
\]
Damit ist die Funktion
\[
x(t)=X(t)x_o = e^{At}x_0
\]
eine L"osung der homogenen Gleichung.

Auch das Verfahren der Variation der Konstanten kann durchgef"uhrt werden.
Dazu ersetzen wir $x_0$ durch einen Vektor $C(t)$, und leiten den Ansatz
\[
x(t)=e^{At}C(t)
\]
nach $t$ ab
\[
\frac{d}{dt}x(t)
=
Ae^{At}C(t) + e^{At}\dot{C}(t)
=
A x(t) + e^{At}\dot{C}(t)
\]
Damit $x(t)$ eine L"osung ist, muss gelten
\[
\begin{aligned}
e^{At}\dot{C}(t)
&=
f(t)
&&\text{und}
&
C(0)0=x_0
\end{aligned}
\]
Diese Differentialgleichung kann durch Integration gel"ost werden,
wir erhalten
\begin{align*}
\dot{C}(t)
&=
e^{-At}f(t)
\\
C(t)
&=
x_0+\int_0^t e^{-A\tau}f(\tau)\,d\tau
\end{align*}
und damit als L"osung der inhomogenen Differentialgleichung
\[
x(t)
=
e^{At}\biggl(
x_0+\int_0^t e^{-A\tau}f(\tau)\,d\tau
\biggr).
\]
Die fr"uher gefundene Formel f"ur die L"osung der Differentialgleichung
(\ref{linear:gleichung}) f"ur den Spezialfall diagonalisierbarer 
Matrizen gilt daher f"ur beliebige Matrizen.

\subsection{Ein wichtiger Spezialfall: Jordan-Normalform}
F"ur einige spezielle Matrizen l"asst sich die Matrix-Exponentialfunktion
direkt berechnen.
Wir beginnen mit folgendem rein algebraischen Resultat, welches wir
sp"ater in einigen n"utzlichen F"allen anwenden.

\begin{definition}
Die Matrix $A(p,q)$ hat die Matrixelemente
\begin{equation}
A(p,q)_{ij}
=
\begin{cases}
0&\qquad i < j-1\\
p&\qquad i = j-1\\
q&\qquad i = j\\
0&\qquad i > j
\end{cases}
\label{linear:apqmatrixelement}
\end{equation}
oder
\begin{equation}
A(p,q)=\begin{pmatrix}
p&q& &      & \\
 &p&q&      & \\
 & &p&\ddots& \\
 & & &\ddots&q\\
 & & &      &p
\end{pmatrix}
\label{linear:apqmatrix}
\end{equation}
\end{definition}

\begin{satz}
\label{linear:apq}
Die Matrix $A(p,q)^k$ hat die Matrixelemente
\begin{equation}
(A(p,q)^k)_{ij}
=\begin{cases}
\binom{k}{j-i}p^{k-(j-i)}q^{j-i} &\qquad i\le j\\
0                                &\qquad i>j
\end{cases}
\label{linear:apqk}
\end{equation}
\end{satz}

\begin{proof}[Beweis]
Wir stellen zun"achst sicher, dass die Formel~\ref{linear:apqk} sinnvoll ist.
F"ur $k=0$ verschwinden alle Binomialkoeffizienten ausser $\binom{k}{0}$,
die Formel~\ref{linear:apqk} beschreibt dann eine Diagonalmatrix, und
die Diagonalelemente sind $\binom{0}{0}p^0q^0=1$, die Formel~\ref{linear:apqk}
beschreibt in diesem Fall also die Einheitsmatrix.

Auch f"ur $k=1$ l"asst sich gleichermassen verifizieren, dass die
Formel~\ref{linear:apqk} in diesem Fall die Matrix $A(p,q)$ beschreibt.
Damit haben wir auch bereits die Verankerung f"ur einen induktiven
Beweis der Formel~\ref{linear:apqk} gelegt.

F"ur den Induktionsschritt nehmen wir jetzt an, dass die
Formel~\ref{linear:apqk} f"ur $k$ bereits bewiesen ist, und berechnen
die Matrixelemente von $A(p,q)^{k+1}$, indem wir das Matrixprodukt
ausrechnen.
Da $A(p,q)$ eine obere Dreiecksmatrix ist, werden auch alle Potenzen
$A(p,q)^k$ obere Dreiecksmatrizen sein, was die Formel~\ref{linear:apqk}
auch ausdr"uckt.
Wir brauchen daher nur Elemente oberhalb der Diagonalen, also f"ur $j\ge i$:
\begin{align*}
(A(p,q)^{k+1})_{ij}
&=
(A(p,q)A(p,q)^k)_{ij}
\\
&=
(\text{Zeile $i$ von $A(p,q)$})
\cdot
(\text{Spalte $j$ von $A(p,q)^k$})
\\
&=
\begin{pmatrix}
0&\dots&p&q&0&\dots
\end{pmatrix}
\begin{pmatrix}
\vdots\\
(A(p,q)^k)_{ij}\\
(A(p,q)^k)_{i+1,j}\\
\vdots
\end{pmatrix}
\\
&=
p(A(p,q)^k)_{ij}
+
q(A(p,q)^k)_{i+1,j}
\\
&=
p\binom{k}{j-i}p^{k-(j-i)}q^{j-i}
+
q\binom{k}{j-(i+1)}p^{k-(j-(i+1))}q^{j-(i+1)}
\\
&=
\left(\binom{k}{j-i}+\binom{k}{j-i-1}\right) p^{k+1-(j-i)}q^{j-i}
\\
&=
\binom{k+1}{j-i} p^{k+1-(j-i)}q^{j-i}
\end{align*}
Damit ist der Induktionsschritt vollzogen und damit die
Formel~\ref{linear:apqk} bewiesen.
\end{proof}

Die Potenzen von $A(p,q)$ k"onnen jetzt mit Hilfe der Exponentialreihe
zu verwenden werden, $e^{A(p,q)}$ zu berechnen:

\begin{satz}
\label{linear:apqexp}
Die Matrixelemente von $e^{A(p,q)}$ sind 
\[
e^{A(p,q)}
=
\begin{cases}
\frac1{(j-i)!}q^{j-i}e^p,
 &\qquad i\le j\\
0&\qquad i>j
\end{cases}
\]
\end{satz}

\begin{proof}[Beweis]
Auch $e^{A(p,q)}$ ist wieder eine obere Dreiecksmatrix, wir brauchen also
nur die Matrixelemente mit $i\le j$ zu berechnen:
\begin{align*}
(e^{A(p,q)})_{ij}
&=
\sum_{k=0}^\infty \frac1{k!}\binom{k}{j-i}p^{k-(j-i)}q^{j-i}
\end{align*}
Wir k"urzen $j-i=s$ ab, und beachten, dass der Binomialkoeffizient nur
dann von $0$ verschieden ist, wenn $k\ge j-i=s$ ist.
Es reicht also, mit der Summe bei $k=s$ zu beginnen
\begin{align*}
(e^{A(p,q)})_{ij}
&=
\sum_{k=s}^\infty \frac1{k!}\binom{k}{s}p^{k-s}q^s
=
\sum_{k=s}^\infty \frac1{k!}\frac{k!}{(k-s)!s!}p^{k-s}q^s
=
\frac1{s!}q^s \sum_{k=s}^\infty \frac{1}{(k-s)!}p^{k-s}
\end{align*}
In der Summe kommt nur die Differenz $k-s$ vor, wir schreiben
daher $r=k-s$ und beginnen die Summe bei $r=0$.
Damit finden wir
\begin{align*}
(e^{A(p,q)})_{ij}
&=
\frac1{s!}q^s \sum_{r=0}^\infty \frac{1}{r!}p^r
=
\frac1{s!}q^se^p,
\end{align*}
womit der Satz bewiesen ist.
\end{proof}

Die S"atze~\ref{linear:apq} und \ref{linear:apqexp} k"onnen dazu verwendet
werden, die Potenzen anderer Matrizen zu berechnen, die f"ur die L"osung von
Differentialgleichungssystemen wichtig sind.

\begin{satz}
\label{linear:jnfexp}
Die Matrix
\[
A(\lambda)=\begin{pmatrix}
\lambda&      1&       &      &       \\
       &\lambda&      1&      &       \\
       &       &\lambda&\ddots&       \\
       &       &       &\ddots&      1\\
       &       &       &      &\lambda
\end{pmatrix}
\]
hat die Matrix-Elemente
\[
a_{ij}(\lambda)=\begin{cases}
\lambda&\qquad j=i\\
      1&\qquad j=i+1\\
      0&\qquad \text{sonst.}
\end{cases}
\]
Dann hat die Matrix $A(\lambda)^k$ die Matrixelemente
\begin{equation}
(A(\lambda)^k)_{ij}=\begin{cases}
\displaystyle \binom{k}{j-i}\lambda^{k-(j-i)}&\qquad i\le j\\
0&\qquad i > j,
\end{cases}
\label{linear:ahochk}
\end{equation}
und $e^{A(\lambda)}$ hat die Matrixelemente
\[
(e^{A(\lambda)})_{ij}
=
\begin{cases}
\frac{1}{(j-i)!}e^\lambda &\qquad i\le j\\
0                         &\qquad i>j.
\end{cases}
\]
\end{satz}

\begin{proof}[Beweis]
Dies folgt unmittelbar aus Satz~\ref{linear:apqk} und \ref{linear:apqexp},
indem man $p=\lambda$ und $q=1$ setzt.
\end{proof}

Nicht jede Matrix $A$ ist diagonalisierbar, aber man kann zeigen, dass
durch Wahl einer geeigneten Basis jede Matrix in eine Form gebracht
werden kann, die aus Bl"ocken der Form $A(\lambda)$ auf der Diagonalen
besteht,
\[
\begin{pmatrix}
A_1(\lambda_1)&              &      &              \\
              &A_2(\lambda_2)&      &              \\
              &              &\ddots&              \\
              &              &      &A_s(\lambda_s)
\end{pmatrix},
\]
wobei die Werte $\lambda_i$ die Eigenwerte der Matrix sind.
Diese Form der Matrix heisst die Jordan-Normalform.
F"ur eine Matrix in Jordan-Normalform kann die Matrix-Exponentialfunktion
explizit berechnet werden, man kann also die L"osungen der
zugeh"origen Differentialgleichung direkt angeben.
Allerdings brauchen wir daf"ur nicht nur $e^{A(\lambda)}$, sondern
$e^{tA(\lambda)}$, wie im folgenden Satz.

\begin{satz}
\label{linear:alambdaexp}
Sei $A(\lambda)$ die Matrix wie in Satz~\ref{linear:jnfexp}.
Dann hat $e^{tA(\lambda)}$ die Matrix-Elemente
\[
(e^{tA(\lambda)})_{ij}
=
\begin{cases}
\frac1{(j-i)!}t^{j-i}e^{\lambda t}
 &\qquad i\le j\\
0&\qquad i>j
\end{cases}
\]
\end{satz}

\begin{proof}[Beweis]
Dies ergibt sich aus Satz~\ref{linear:apqexp}, indem man $p=\lambda t$ und
$q=t$ setzt.
\end{proof}

Ausgeschrieben ist dies
\[
e^{tA(\lambda)}
=
\begin{pmatrix}
e^{\lambda t}&te^{\lambda t}&\frac12t^2e^{\lambda t}&      & \\
             & e^{\lambda t}&te^{\lambda t}         &\ddots& \\
             &              &e^{\lambda t}          &\ddots&\frac12t^2e^{\lambda t}\\
             &              &                       &\ddots&te^{\lambda t}\\
             &              &                       &      &e^{\lambda t}
\end{pmatrix}.
\]

\begin{beispiel}
Als Beispiel betrachten wir die homogene Differentialgleichung
\[
y''+2\sigma y'+\sigma^2y=0.
\]
Als Erstes bringen wird diese Gleichung in Vektorform 
\[
Y(x)=\begin{pmatrix}
y(x)\\y'(x)
\end{pmatrix}
\qquad\Rightarrow\qquad
\frac{d}{dx}Y(x)
=
\begin{pmatrix}
y'(x)\\
-2\sigma y'(x)-\sigma^2y(x)
\end{pmatrix}
=
\begin{pmatrix}
0&1\\
-\sigma^2& -2\sigma
\end{pmatrix}
Y(x).
\]
Die Matrix auf der rechten Seite hat das charakteristische Polynom
\begin{align*}
\left|\,\begin{matrix}
—\lambda&1\\
-\sigma^2&-\lambda-2\sigma
\end{matrix}\,\right|
&=
\lambda(\lambda+2\sigma)+\sigma^2
=
\lambda^2+2\sigma\lambda+\sigma^2=(\lambda+\sigma)^2=0
\\
\Rightarrow\qquad
\lambda&=-\sigma
\end{align*}
Das charakteristische Polynom hat also nur eine Nullstelle.
Wir bestimmen die Eigenvektoren:
\begin{align*}
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
-\lambda & 1\\
-\sigma^2&-\lambda-2\sigma\\
\hline
\end{tabular}
&=
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
\sigma & 1\\
-\sigma^2&\sigma-2\sigma\\
\hline
\end{tabular}
=
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
\sigma & 1\\
-\sigma^2&-\sigma\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
1&\frac1{\sigma}\\
0&0\\
\hline
\end{tabular}
\qquad\Rightarrow\qquad
v=
\begin{pmatrix}
1\\-\sigma
\end{pmatrix}
\end{align*}
Da es nur einen Eigenvektor gibt, kann die Matrix nicht diagonalisierbar
sein.
Um eine Basis zu konstruieren, in welcher die Matrix Jordan-Normalform
bekommt, brauchen wir eine Vektor $w$, der die Gleichung
\[
Aw = \lambda w + v
\qquad\Rightarrow\qquad
(A-\lambda E)w=w
\]
erf"ullt.
Setzen wir die bekannten Werte ein, suchen wir eine L"osung des
Gleichungssystems
\[
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
 \sigma  &    1    & 1 \\
-\sigma^2& -\sigma & -\sigma\\
\hline
\end{tabular}
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|>{$}c<{$}|}
\hline
 \sigma  &    1    & 1 \\
     0   &    0    & 0 \\
\hline
\end{tabular}
\qquad
\Rightarrow
\qquad
w=\begin{pmatrix}
0\\1
\end{pmatrix}
\]
wobei wir f"ur die frei w"ahlbare zweite Variable den Wert $1$ gew"ahlt haben.
In der Basis $B'=\{v, w\}$ hat die Matrix des Differentialgleichungssystems
die Form
\[
A'
=
\begin{pmatrix}
-\sigma&      1\\
      0&-\sigma 
\end{pmatrix}
\]
Die Transformationsmatrix, die von der Standardbasis in die Basis $B'$
umrechnet ist
\[
T
=
\begin{pmatrix}
      1&0\\
-\sigma&1
\end{pmatrix}
\qquad\text{und}\qquad
T^{-1}
=
\begin{pmatrix}
     1&0\\
\sigma&1
\end{pmatrix}.
\]
Tats"achlich kann man nachrechnen, dass
\[
T^{-1}AT
=
\begin{pmatrix}
     1&0\\
\sigma&1
\end{pmatrix}
\begin{pmatrix}
0&1\\
-\sigma^2&-2\sigma
\end{pmatrix}
\begin{pmatrix}
      1&0\\
-\sigma&1
\end{pmatrix}
=
\begin{pmatrix}
     1&0\\
\sigma&1
\end{pmatrix}
\begin{pmatrix}
-\sigma  &1\\
 \sigma^2&-2\sigma
\end{pmatrix}
=
\begin{pmatrix}
-\sigma&1\\
      0&-\sigma
\end{pmatrix}.
\]

Um die Differentialgleichung zu l"osen, brauchen wir die Matrix
$e^{A'x}$, die wir aus Satz~\ref{linear:alambdaexp} ablesen k"onnen:
\begin{align*}
e^{A'x}
&=
\begin{pmatrix}
e^{-\sigma x}&xe^{-\sigma x}\\
       0     & e^{-\sigma x}
\end{pmatrix}
=
\begin{pmatrix}
1&x\\
0&1
\end{pmatrix}
e^{-\sigma x}.
\end{align*}
Diese Matrix transformieren wir mit 
\begin{align*}
Te^{A'x}T^{-1}
&=
e^{-\sigma x}
\begin{pmatrix}
      1&0\\
-\sigma&1
\end{pmatrix}
\begin{pmatrix}
1&x\\
0&1
\end{pmatrix}
\begin{pmatrix}
     1&0\\
\sigma&1
\end{pmatrix}
=
e^{-\sigma x}
\begin{pmatrix}
      1&0\\
-\sigma&1
\end{pmatrix}
\begin{pmatrix}
1+\sigma x&x\\
  \sigma  &1
\end{pmatrix}
\\
&=
e^{-\sigma x}
\begin{pmatrix}
1+\sigma x& x\\
-\sigma-\sigma^2 x+\sigma&-\sigma x+1
\end{pmatrix}
=
e^{-\sigma x}
\begin{pmatrix}
1+\sigma   x&         x\\
 -\sigma^2 x&1-\sigma x
\end{pmatrix}.
\end{align*}
Und man kann nachrechnen, dass $X(x)$ die Matrix-Differentialgleichung
\[
\frac{d}{dx}X(x)=AX(x)
\]
erf"ullt.
Daher kann man jetzt auch die L"osung der urspr"unglichen 
Differentialgleichung angeben:
\[
y(x)=e^{-\sigma x}(1+\sigma x)y(0) + e^{-\sigma x}xy'(0)
\]
zu Anfangsbedingungen $y(0)$ und $y'(0)$.
\end{beispiel}

\subsection{Harmonischer Oszillator\label{linear:harmosz}}
Ein ged"ampfter harmonischer Oszillator wird durch die lineare
Differentialgleichung
\begin{equation}
\ddot X=-\lambda^2 X-b\dot X,\qquad X(0)=X_0,\qquad \dot X(0)=X_1.
\label{linear:harmosz-dgl2}
\end{equation}
zweiter Ordnung beschrieben.
Als Differentialgleichungssystem erster Ordnung geschrieben wird dies zu
\begin{equation}
\begin{aligned}
y_1'&=y_2,                              &&&y_1(0)&=x_0, \\
y_2'&=-\lambda^2 y_1-by_2 + \sigma w(x),&&&y_2(0)&=x_1.
\end{aligned}
\label{stochastsich:harmosz-dgl1}
\end{equation}

Wie bei den gew"ohnlichen Differentialgleichungen k"onnen wir diese
Gleichung einfacher l"osen, wenn wir sie in Matrixform schreiben:
\begin{equation}
\frac{d}{dx} \begin{pmatrix}y_1\\y_2\end{pmatrix}
=
\underbrace{
\begin{pmatrix}
         0& 1\\
-\lambda^2&-b
\end{pmatrix}}_{\textstyle=D}
\begin{pmatrix}y_1\\y_2\end{pmatrix}
+
\begin{pmatrix}
0\\\sigma
\end{pmatrix}w(x)
\end{equation}
Die L"osung wird dann
\begin{equation}
\begin{pmatrix}
y_1(x)\\y_2(x)
\end{pmatrix}
=
e^{Dx}\begin{pmatrix}x_0\\x_1\end{pmatrix}
+
\sigma \int_0^x e^{D(x-\xi)}\begin{pmatrix}0\\\sigma\end{pmatrix}w(\xi)\,d\xi.
\label{linear:harmosz-explsg}
\end{equation}

\begin{beispiel}
Wir berechnen die L"osung f"ur den Spezialfall $b=0$.
Die Matrix $D$ ist
\[
D=\begin{pmatrix}
0&1\\-\lambda^2&0
\end{pmatrix}.
\]
Die Matrix $e^{Dx}$ kann durch Diagonalisierung berechnet werden.
Die Transformationsmatrix
\[
T
=
\begin{pmatrix}
1&1\\
i\lambda&-i\lambda
\end{pmatrix},
\qquad
T^{-1}
=
\frac12
\begin{pmatrix}
1& 1/i\lambda\\
1&-1/i\lambda
\end{pmatrix}
\]
bringt die Matrix $D$ in Diagonalform:
\[
T^{-1}DT
=
\begin{pmatrix}
i\lambda&        0\\
        &-i\lambda
\end{pmatrix}.
\]
Daraus kann man jetzt die Exponentialfunktion berechnen:
\begin{align*}
T^{-1}e^{Dx}T
&=
\begin{pmatrix}
\cos\lambda x+i\sin\lambda x&              0             \\
              0             &\cos\lambda x-i\sin\lambda x
\end{pmatrix}
\\
\Rightarrow\qquad\qquad
e^{Dx}
&=
\begin{pmatrix}
                \cos\lambda x&\frac1{\lambda}\sin\lambda x\\
-\frac1{\lambda}\sin\lambda x&               \cos\lambda x
\end{pmatrix}
\end{align*}
Daraus k"onnen wir jetzt die L"osung mit der
Formel~(\ref{linear:harmosz-explsg}) ablesen:
\begin{equation}
\begin{pmatrix}
y_1(x)\\y_2(x)
\end{pmatrix}
=
\begin{pmatrix}
                \cos\lambda x&\frac1{\lambda}\sin\lambda x\\
-\frac1{\lambda}\sin\lambda x&               \cos\lambda x
\end{pmatrix}
\begin{pmatrix}x_0\\x_1\end{pmatrix}
+
\sigma\int_0^t
\begin{pmatrix}
\frac1{\lambda}\sin\lambda(t-\tau)\\
               \cos\lambda(t-\tau)
\end{pmatrix}\,dW.
\end{equation}
Die erste Komponenten davon ist
\begin{equation}
y_1(x)
=
x_0\cos\lambda x+\frac{x_1}{\lambda}\sin\lambda x
+
\frac{\sigma}{\lambda}\int_0^x\sin\lambda(x-\xi)w(\xi)\,d\xi,
\label{linear:harmosz-y}
\end{equation}
die L"osung der Differentialgleichung.
\end{beispiel}

\section{Zeitabh"angige Koeffizienten}
Die L"osung der Differentialgleichung~(\ref{linear:gleichung}) war
deshalb einfach, weil wir die Matrixdifferentialgleichung
\[
\frac{d}{dt} X=AX
\qquad
\text{mit}
\qquad
X(0)=E
\]
mit der Exponentialfunktion sofort l"osen k"onnten.
F"ur eine beliebige Funktion $A(t)$ k"onnen wir nicht mehr mit der
Exponentialfunktion arbeiten.
Wir k"onnen uns aber daran erinnern, dass die Exponentialfunktion
eine L"osung einer Matrixdifferentialgleichung war.
Wir suchen daher $X(t)$ als L"osung der Differentialgleichung
\[
\frac{d}{dt}X(t)=A(t)X(t)
\qquad\text{und}\qquad
X(0)=E.
\]
Die inverse Matrix $X(t)^{-1}$ kann man ebenfalls als L"osung einer
Differentialgleichung finden.
Die Ableitung der Beziehung
\[
X(t)X(t)^{-1}=E
\]
ist
\[
0
=
\frac{d}{dt}X(t)X(t)^{-1}
=
\frac{d}{dt} X(t) X(t)^{-1}
+
X(t) \frac{d}{dt}X(t)^{-1}
=
A(t)X(t)X^{-1}(t)
+
X(t) \frac{d}{dt}X(t)^{-1}
=
A(t)+ X(t)\frac{d}{dt}X(t)^{-1}
\]
Die Inverse $X(t)^{-1}$ erf"ullt daher die Differentialgleichung
\[
\frac{d}{dt}X(t)^{-1}= X(t)^{-1}A(t).
\]
Die Inverse kann also genauso als L"osung einer Differentialgleichung
gefunden werden wie $X(t)$.

\subsection{L"osung der Matrixdifferentialgleichung}
Die lineare homogene Gleichung erste Ordnung
\[
y'(x)=a(x)y(x)
\]
kann mit Separation gel"ost werden:
\begin{align*}
\frac{dy}{dx}&=a(x)y(x)
\\
\int\frac{dy}{y(x)}&=\int a(x)\,dx+C
\\
\log y &= \int a(x)\,dx + C
\\
y(x)&=Ke^{\int a(x)\,dx}.
\end{align*}
Nat"urlich sind diese rein formalen Manipulationen nicht auf die
Matrixsituation "ubertragbar, aber wir k"onnen versuchen, das Resultat
durch Analogie zu "ubertragen.
Wir vermuten daher, dass die Funktion
\begin{equation}
X(t)=e^{\int_0^tA(\tau)\,d\tau}
\label{linear:nichtkommutativ-ansatz}
\end{equation}
eine L"osung der Matrix-Differentialgleichung
\begin{equation}
\frac{d}{dt}X(t)=A(t)X(t)\qquad\text{und}\qquad X(0)=E
\label{linear:nichtkommutativ-gleichung}
\end{equation}
ist.
Wir pr"ufen dies nach, indem wir den
Ansatz~(\ref{linear:nichtkommutativ-ansatz})
in die Gleichung~(\ref{linear:nichtkommutativ-gleichung}) einsetzen.
Die Anfangsbedingung ist selbstverst"andlich erf"ullt.
F"ur die Ableitung verwenden wir die Definition der Ableitung als
Grenzwert eines Differenzenquotienten:
\begin{align}
\frac{d}{dt}X(t)
&=
\lim_{h\to 0}\frac1h\biggl(e^{\int_0^{t+h}A(\tau)\,d\tau}
-
e^{\int_0^tA(\tau)\,d\tau}\biggr)
\notag
\\
&=
\lim_{h\to 0}\frac1h\biggl(
e^{\int_0^tA(\tau)\,d\tau
+
\int_t^{t+h}A(\tau)\,d\tau}
-
e^{\int_0^tA(\tau)\,d\tau}\biggr)
\label{linear:nichtkommutativ-diffquot}
\end{align}
An dieser Stelle m"ochten wir gerne eine Identit"at der Form $e^{B+C}=e^Be^C$
verwenden, die uns erlauben w"urde, den gemeinsamen Term
$e^{\int_0^tA(\tau)\,d\tau}=X(t)$ auszuklammern.
F"ur die Exponentialfunktion von Zahlen ist die Identit"at eine
Selbstverst"andlichkeit, aber leider nicht f"ur die Matrizen.
Denn w"are $e^{B+C}=e^Be^C$, dann m"usste auch $e^{B+C}=e^{C+B}=e^Ce^B$
sein, die Matrizen $e^B$ und $e^C$ m"ussten vertauschen.
Dies ist im allgemeinen aber nicht der Fall:

\begin{satz}[Baker-Campbell-Hausdorff]
F"ur zwei beliebige $n\times n$-Matrizen $B$ und $C$ gilt
\[
[e^B,e^C]=[B,C]+\frac12([B,C^2]+[B^2,C])\dots
\]
wobei $[X,Y]=XY-YX$ der {\em Kommutator} von $X$ und $Y$ ist.
\index{Kommutator}
Insbesondere vertauschen $e^B$ und $e^C$ genau dann, wenn $B$ und $C$
vertauschen.
\end{satz}

\begin{proof}[Beweis]
Wir setzen die Reihenentwicklung von $e^B$ und $e^C$ in den Kommutator
ein:
\begin{align*}
e^B
&=
E+B+\frac12B^2+\frac1{3!}B^3+\dots
\\
e^C
&=
E+C+\frac12C^2+\frac1{3!}C^3+\dots
\\
[e^B,e^C]
&=
e^Be^C-e^Ce^B
\\
&=
\biggl(E+B+\frac12B^2+\frac1{3!}B^3+\dots\biggr)
\biggl(E+C+\frac12C^2+\frac1{3!}C^3+\dots\biggr)
\\
&\qquad
-
\biggl(E+C+\frac12C^2+\frac1{3!}C^3+\dots\biggr)
\biggl(E+B+\frac12B^2+\frac1{3!}B^3+\dots\biggr)
\\
&=
\biggl(E+C+B+BC+\frac12C^2+\frac12B^2+\frac12BC^2+\frac12B^2C+\dots\biggr)
\\
&\qquad
-
\biggl(E+B+C+CB+\frac12B^2+\frac12C^2+\frac12CB^2+\frac12C^2B+\dots\biggr)
\\
&=
BC-CB+\frac12(BC^2+B^2C-CB^2-C^2B)+\dots
\\
&=[B,C]+\frac12([B,C^2]+[B^2,C])+\dots
\end{align*}
\end{proof}
Wenn allerdings die Matrizen
\[
e^{\int_0^tA(\tau)\,d\tau}
\qquad\text{und}\qquad
e^{\int_t^{t+h}A(\tau)\,d\tau}
\]
vertauschen, dann l"asst sich die Berechnung
(\ref{linear:nichtkommutativ-diffquot}) weiterf"uhren, wir
erhalten
\begin{align*}
\frac{d}{dt}X(t)
&=
\lim_{h\to 0}\frac1h\biggl(
e^{\int_t^{t+h}A(\tau)\,d\tau}
-
E
\biggr)
\cdot
e^{\int_0^tA(\tau)\,d\tau}
\\
&=
\lim_{h\to0}\frac1h
\biggl(E+\int_t^{t+h}A(\tau)\,d\tau+\dots-E\biggr)\cdot X(t)
=A(t)X(t).
\end{align*}
Die Bedingung, dass die Matrizen $A(t)$ vertauschen m"ussen, ist
ziemlich strikt, sie ist aber nat"urlich erf"ullt, wenn $A(t)$
konstant ist.
Sie ist auch erf"ullt f"ur Matrizen in den folgenden zwei
Familien.
\begin{align*}
A_1(s)&=\begin{pmatrix}1&s\\0&1\end{pmatrix}\\
A_2(\lambda)&=\begin{pmatrix}\lambda&1\\0&\lambda\end{pmatrix}\\
A_3(\lambda)&=\begin{pmatrix}\lambda&      1&      &       \\
                                   0&\lambda&     1&       \\
                                    &       &\ddots&\ddots \\
                                    &       &      &\lambda\end{pmatrix}\\
A_3(\alpha)&=\begin{pmatrix}\cos \alpha&\sin \alpha\\-\sin \alpha&\cos \alpha\end{pmatrix}
\end{align*}
F"ur $A_4$ ist dies klar, denn dies sind Drehmatrizen in der Ebene,
die schon aus geometrischen Gr"unden vertauschen.
F"ur $A_1$ rechnen wir nach:
\begin{align*}
A_1(s_1)A_1(s_2)
&=
\begin{pmatrix}1&s_1\\0&1\end{pmatrix}
\begin{pmatrix}1&s_2\\0&1\end{pmatrix}
=
\begin{pmatrix}1&s_1+s_2\\0&1\end{pmatrix}
=
A_1(s_1+s_2),
\end{align*}
woraus sofort
$
A_1(s_1)A_1(s_2)=
A_1(s_2)A_1(s_1)
$
folgt.
Analog kann man die Vertauschung der Matrizen $A_2$ und $A_3$ untereinander
nachrechnen.

\subsection{L"osung der inhomogenen Gleichung}
Wenn es gelingt, die Matrix-Differentialgleichung zu l"osen, dann
ist $x(t)=X(t)x_0$ wieder eine L"osung der homogenen Gleichung.
Ausserdem k"onnen wir wieder versuchen, die L"osung der inhomogenen
Gleichung mit Variation der Konstanten zu finden.
Dazu setzen wir $x(t)=X(t)C(t)$ an, und berechnen die Ableitung
\[
\frac{d}{dt}x(t)
=
\dot{X}(t)C(t)+X(t)\dot{C}(t)
=
A(t)X(t)C(t)+X(t)\dot{C}(t)
=
A(t)x(t)+X(t)\dot{C}(t)
\]
Wieder muss $X(t)\dot{C}(t)=f(t)$ gelten, wenn $x(t)$ eine L"osung sein
soll.
Die Gleichung
\[
\frac{d}{dt}C(t)
=
X(t)^{-1}f(t)
\]
Kann durch Integration gel"ost werden:
\[
C(t)
=x_0+\int_0^t X(t)^{-1}(\tau)f(\tau)\,d\tau,
\]
so dass die L"osung der inhomogenen Differentialgleichung
\[
x(t)=X(t)\biggl(x_0+\int_0^t X(\tau)^{-1} f(\tau)\,d\tau\biggr)
\]
wird.
